# GLPI AI Automation

## üéØ Descripci√≥n del Proyecto

Este proyecto busca modernizar y automatizar la plataforma open-source GLPI. La meta es integrar un sistema de agentes de IA para automatizar las respuestas del soporte t√©cnico, optimizar los flujos de trabajo y enriquecer la experiencia de usuario, especialmente para personas sin perfil t√©cnico.

## üèóÔ∏è Arquitectura y Visi√≥n General
El n√∫cleo del sistema est√° dise√±ado en torno a un flujo de trabajo que se activa con la creaci√≥n de un ticket en GLPI. La informaci√≥n del ticket es procesada por un sistema de agentes inteligentes (`CrewAI`) que colaboran para analizar, enriquecer y proponer soluciones.

Este `CrewAI` se conecta a herramientas externas (bases de conocimiento, sistemas de monitorizaci√≥n) a trav√©s de un **Servidor MCP (Model Context Protocol)**, que act√∫a como un bus de datos.

### Flujo de Trabajo del Ticket
![Diagrama de Arquitectura V3](https://raw.githubusercontent.com/ANFAIA/GLPI-AssistIA/c81ce359886bd2f5c9111d7a7446144947432ea3/docs/BigPicture%20V3.svg)

1.  **Entrada en GLPI**: Un usuario o t√©cnico crea un ticket de incidencia.
2.  **Activaci√≥n del Agente**: La incidencia se transfiere al sistema `CrewAI` para su procesamiento.
3.  **An√°lisis por Agentes IA**:
      * **Analista de Emociones**: Eval√∫a la urgencia y el estado de √°nimo del usuario para priorizar el ticket.
      * **Agente Categorizador**: Clasifica la incidencia seg√∫n las etiquetas predefinidas en GLPI.
      * **Agente GLPI**: Busca en el historial de GLPI incidencias similares o relacionadas para obtener contexto.
      * **Agente Experto en [X]**: Para cada categor√≠a, un agente especializado consulta bases de datos externas como **Wiki.js**, **Zabbix**, etc., a trav√©s del Servidor MCP.
      * **Agente Resolutor**: Consolida toda la informaci√≥n, genera un resumen enriquecido y una posible soluci√≥n.
4.  **Respuesta en GLPI**: La soluci√≥n y el an√°lisis generados se publican en el ticket de GLPI, asistiendo al t√©cnico o respondiendo directamente al usuario.

## ‚ú® Caracter√≠sticas Principales

  * **Resumen y Enriquecimiento de Tickets**: La IA analiza y resume el problema del usuario, a√±adiendo contexto t√©cnico.
  * **Generaci√≥n de Plantillas**: Creaci√≥n autom√°tica de plantillas de respuesta t√©cnica.
  * **Integraci√≥n con Herramientas de Diagn√≥stico**: Conexi√≥n con **Zabbix**, **Wazuh** y **Wiki.js** para obtener datos en tiempo real.
  * **Arquitectura MCP**: Un bus de datos desacoplado para facilitar la comunicaci√≥n y la escalabilidad.
  * **Informaci√≥n Contextual Inteligente**: Proporciona informaci√≥n relevante tanto a t√©cnicos como a usuarios.

## üìä M√©tricas de √âxito

El √©xito del proyecto se medir√° por la consecuci√≥n de los siguientes objetivos:

  * Reducci√≥n de m√°s del **70%** en el tiempo de primera respuesta.
  * Precisi√≥n superior al **85%** en las respuestas autom√°ticas generadas.
  * Reducci√≥n de m√°s del **50%** en los tickets que necesitan ser escalados manualmente.
  * Reducci√≥n de m√°s del **40%** en el tiempo promedio de resoluci√≥n de incidencias.
  * Nivel de satisfacci√≥n del usuario superior a **4.0/5.0**.


## ü§ñ M√≥dulo de agentes (Glpi AssistIA Server)

Serve como backend que:
1. Recibe eventos de GLPI (por ejemplo, creaci√≥n o actualizaci√≥n de tickets).
2. Orquesta los agentes de IA a trav√©s del servidor MCP (`mcp_server.py` en la ra√≠z).
3. Devuelve al ticket un resumen enriquecido o soluci√≥n.

> Nota: El servidor MCP est√° en la ra√≠z del repo (`mcp_server.py`). Esta versi√≥n 3 sustituye a los prototipos previos (V1/V2), que ya no est√°n en el repositorio.

--

### Requisitos
- **GLPI** versi√≥n 10.x o superior (acceso v√≠a API).
- **Credenciales GLPI:** `GLPI_URL`, `GLPI_APP_TOKEN` y `GLPI_USER_TOKEN`.
- **Servidor AssistIA** (`glpiassistiaserver/`):
  - Si es **PHP**, requiere **PHP ‚â• 8.1** y **Composer**.
  - Si es **Python**, requiere **Python ‚â• 3.11**.
- **MCP (opcional pero recomendado):** `python ‚â• 3.11` para ejecutar `mcp_server.py`.
- **(Opcional)** LLM local si la orquestaci√≥n usa modelos locales (por ejemplo, Ollama).

---

### Variables de entorno
Crea un archivo `.env` que contenga:

```env
# GLPI
GLPI_URL=https://tu-glpi.example.com
GLPI_API_URL=https://mi.glpi/apirest.php
GLPI_APP_TOKEN=xxx
GLPI_USER_TOKEN=yyy
GLPI_VERIFY_SSL=true

# MCP
MCP_HOST=127.0.0.1
MCP_PORT=8765

# LLM/Ollama (si aplica)
OLLAMA_HOST=http://127.0.0.1:11434

# Wiki.js
WIKIJS_URL=http://localhost:8080/
WIKIJS_API_TOKEN=tu_token

````

---

### Puesta en marcha

1. **Arranca el MCP** desde la ra√≠z del repo:

   ```bash
   python -m pip install -r requirements.txt
   python mcp_server.py
   ```

2. **Arranca el servidor AssistIA** desde `glpiassistiaserver/`:

   * **PHP (Composer):**

     ```bash
     cd glpiassistiaserver
     composer install
     # Si hay carpeta public/:
     php -S 127.0.0.1:8080 -t public
     ```
   * **Python (FastAPI / Flask, etc.):**

     ```bash
     cd glpiassistiaserver
     python -m pip install -r requirements.txt
     uvicorn app:app --host 0.0.0.0 --port 8080 --reload
     ```

---

### Integraci√≥n con GLPI (EN OBRAS)

1. En GLPI ‚Üí Configuraci√≥n ‚Üí API:

   * Habilita la API y genera los tokens.
2. Configura un **webhook** (o tarea equivalente) que apunte al endpoint del servidor AssistIA, por ejemplo:

   ```
   POST http://TU_ASSISTIA:8080/webhook/glpi
   ```

El servidor validar√° el evento, invocar√° MCP y publicar√° el resultado en el ticket.

### Versi√≥n 3 (Implementaci√≥n CrewAI como paquete instalable)

El proyecto usa [uv](https://docs.astral.sh/uv/getting-started/installation/) para el desarrollo.

Una vez instalada la utilidad, puedes ejecutar el siguiente comando para ejecutar la interfaz de l√≠nea de comandos:

```bash
uv run glpiassistiaserver-cli
```

Para lanzar el servidor HTTP ejecuta:

```bash
uvicorn glpiassistiaserver.webapp:app --reload
```

üß† Documento realizado por un humano y potenciado por IA
