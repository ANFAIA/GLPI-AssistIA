# GLPI AI Automation

## üéØ Descripci√≥n del Proyecto

Este proyecto busca modernizar y automatizar la plataforma open-source GLPI. La meta es integrar un sistema de agentes de IA para automatizar las respuestas del soporte t√©cnico, optimizar los flujos de trabajo y enriquecer la experiencia de usuario, especialmente para personas sin perfil t√©cnico.

## üèõÔ∏è Arquitectura y Visi√≥n General
El n√∫cleo del sistema est√° dise√±ado en torno a un flujo de trabajo que se activa con la creaci√≥n de un ticket en GLPI. La informaci√≥n del ticket es procesada por un sistema de agentes inteligentes (`CrewAI`) que colaboran para analizar, enriquecer y proponer soluciones.

Este `CrewAI` se conecta a herramientas externas (bases de conocimiento, sistemas de monitorizaci√≥n) a trav√©s de un **Servidor MCP (Model Context Protocol)**, que act√∫a como un bus de datos.

### Flujo de Trabajo del Ticket
![Diagrama de Arquitectura V3](https://raw.githubusercontent.com/ANFAIA/GLPI-AssistIA/c81ce359886bd2f5c9111d7a7446144947432ea3/docs/BigPicture%20V3.svg)

1.  **Entrada en GLPI**: Un usuario o t√©cnico crea un ticket de incidencia.
2.  **Activaci√≥n del Agente**: La incidencia se transfiere al sistema `CrewAI` para su procesamiento.
3.  **An√°lisis por Agentes IA**:
      * **Analista de Emociones**: Eval√∫a la urgencia y el estado de √°nimo del usuario para priorizar el ticket.
      * **Agente Categorizador**: Clasifica la incidencia seg√∫n las etiquetas predefinidas en GLPI.
      * **Agente GLPI**: Busca en el historial de GLPI incidencias similares o relacionadas para obtener contexto.
      * **Agente Experto en \[X]**: Para cada categor√≠a, un agente especializado consulta bases de datos externas como **Wiki.js**, **Zabbix**, etc., a trav√©s del Servidor MCP.
      * **Agente Resolutor**: Consolida toda la informaci√≥n, genera un resumen enriquecido y una posible soluci√≥n.
4.  **Respuesta en GLPI**: La soluci√≥n y el an√°lisis generados se publican en el ticket de GLPI, asistiendo al t√©cnico o respondiendo directamente al usuario.

## ‚ú® Caracter√≠sticas Principales

  * **Resumen y Enriquecimiento de Tickets**: La IA analiza y resume el problema del usuario, a√±adiendo contexto t√©cnico.
  * **Generaci√≥n de Plantillas**: Creaci√≥n autom√°tica de plantillas de respuesta t√©cnica.
  * **Integraci√≥n con Herramientas de Diagn√≥stico**: Conexi√≥n con **Zabbix**, **Wazuh** y **Wiki.js** para obtener datos en tiempo real.
  * **Arquitectura MCP**: Un bus de datos desacoplado para facilitar la comunicaci√≥n y la escalabilidad.
  * **Informaci√≥n Contextual Inteligente**: Proporciona informaci√≥n relevante tanto a t√©cnicos como a usuarios.

## üìä M√©tricas de √âxito

El √©xito del proyecto se medir√° por la consecuci√≥n de los siguientes objetivos:

  * Reducci√≥n de m√°s del **70%** en el tiempo de primera respuesta.
  * Precisi√≥n superior al **85%** en las respuestas autom√°ticas generadas.
  * Reducci√≥n de m√°s del **50%** en los tickets que necesitan ser escalados manualmente.
  * Reducci√≥n de m√°s del **40%** en el tiempo promedio de resoluci√≥n de incidencias.
  * Nivel de satisfacci√≥n del usuario superior a **4.0/5.0**.


## ü§ñ Prototipo de Agentes
### VERSI√ìN 1 (Carpeta Old)

**Nota**: Este prototipo est√° dise√±ado para funcionar sin credenciales reales de GLPI, usando datos simulados para demostrar la funcionalidad de generaci√≥n de res√∫menes con IA. 


#### Requisitos de Software

##### 1. Python 3.x
- **Versi√≥n recomendada**: Python 3.11 o superior
- **Incluye**: pip para instalar dependencias

##### 2. Ollama (Programa externo)
- **Estado**: Debe estar instalado y ejecut√°ndose
- **Modelo requerido**: `phi3:mini`
- **Puerto por defecto**: 11434
- **Comandos necesarios**:
  ```bash
  ollama serve          # Iniciar el servidor
  ollama pull phi3:mini # Descargar el modelo
  ```

##### 3. Dependencias de Python
Archivo `requirements.txt`:
```
python-dotenv==1.0.0
requests==2.31.0
```


#### Orden de Instalaci√≥n

1. **Instalar Python 3.x**
   - Descargar desde [python.org](https://python.org)
   - Verificar instalaci√≥n: `python --version`

2. **Instalar Ollama**
   - Descargar desde [ollama.ai](https://ollama.ai)
   - Verificar instalaci√≥n: `ollama --version`

3. **Descargar modelo de IA**
   ```bash
   ollama pull phi3:mini
   ```

4. **Instalar dependencias de Python**
   ```bash
   pip install -r requirements.txt
   ```

5. **Configurar variables de entorno** (opcional)
   - Crear archivo `.env` con la configuraci√≥n de GLPI

6. **Ejecutar el agente**
   ```bash
   python src/agent.py
   ```

   ---
### Versi√≥n 2 (Implementaci√≥n CrewAI)

Este documento detalla los pasos necesarios para configurar y ejecutar el sistema de agentes inteligentes basado en **CrewAI**. El sistema est√° dise√±ado para analizar, clasificar y proponer soluciones a incidencias de soporte t√©cnico.

#### 1\. Requisitos Previos

Antes de empezar, aseg√∫rate de tener instalado y configurado lo siguiente:

##### Python

Es necesario tener instalado Python en tu sistema con las siguientes dependencias:
    ```bash
    pip install crewai langchain-community python-dotenv requests
    ```
##### Ollama

El motor de los agentes funciona con modelos de lenguaje ejecutados localmente a trav√©s de Ollama.

1.  **Instala Ollama**: Sigue las instrucciones de instalaci√≥n desde [ollama.ai](https://ollama.ai).
2.  **Descarga los modelos necesarios**: Este proyecto utiliza modelos espec√≠ficos para cada agente con el fin de optimizar el rendimiento. Ejecuta los siguientes comandos para descargarlos:
    ```bash
    ollama pull qwen3
    ollama pull deepseek-coder
    ollama pull deepseek-r1
    ```
3.  **Inicia el servidor de Ollama**: Aseg√∫rate de que Ollama se est√© ejecutando en segundo plano. Por defecto, estar√° disponible en `http://localhost:11434`.

#### 2\. Configuraci√≥n

Sigue estos pasos para configurar el entorno de ejecuci√≥n:

##### Archivo de Incidencia

El script principal (`main_demo.py`) lee la incidencia a analizar desde un fichero de texto. En pr√≥ximas veriones esta tarea ser√° realizada por main.py, que leera la incidencia y proporcionar√° el informe directamente en GLPI.

1.  Aseg√∫rate de que exista un archivo llamado **`incidencia.txt`** en la carpeta `CrewAi`.
2.  El contenido de este archivo debe ser la descripci√≥n de la incidencia que quieres que los agentes analicen. Puedes usar el siguiente ejemplo:
    ```
    T√çTULO: Usuario no puede acceder a la red corporativa desde su port√°til

    DESCRIPCI√ìN INICIAL:
    El usuario Juan P√©rez (juan.perez@empresa.com) reporta que desde esta ma√±ana no puede conectarse a la red corporativa desde su port√°til Dell Latitude 5520. El equipo muestra el mensaje "No se puede conectar a esta red" cuando intenta conectarse al WiFi de la oficina. El usuario confirma que la contrase√±a es correcta y que otros dispositivos en la misma ubicaci√≥n funcionan normalmente.
    ```

##### Herramientas (Tools)

El `buscador_soluciones` utiliza herramientas para diagnosticar problemas.

  * **Wiki.js Tool**: Si deseas conectar el agente a una base de conocimiento de Wiki.js, debes editar el archivo `CrewAi/tools/wikijs_tool.py` y configurar las variables `WIKIJS_URL` y `WIKIJS_API_TOKEN` con tus credenciales.
  * **Ping Tool**: Si deseas que el agente realice un ping a una p√°gina, deber√°s de indicarlo en el campo habilitado. Por defecto est√° establecida una direcci√≥n gen√©rica.

#### 3\. Ejecuci√≥n

Una vez completados los requisitos y la configuraci√≥n, puedes ejecutar el sistema de agentes.

1.  Navega hasta la carpeta `CrewAi` en tu terminal.
2.  Ejecuta el script de demostraci√≥n:
    ```bash
    python main_demo.py
    ```

El script iniciar√° el "Crew", que procesar√° la incidencia de manera secuencial a trav√©s de sus agentes. Ver√°s en la terminal el razonamiento de cada agente y el resultado final, que ser√° un informe guardado en `informe_soluciones.md`.

#### 4\. Arquitectura de los Agentes

El sistema se compone de tres agentes especializados, cada uno con un modelo de lenguaje recomendado para su tarea espec√≠fica (En la documentaci√≥n se dispone de un an√°lisis detallado):

  * **`analista_sentimiento`**:
      * **Objetivo**: Analizar el estado emocional y la urgencia del cliente.
      * **Modelo recomendado**: **`Qwen3`**, por su capacidad para el an√°lisis profundo de matices en el lenguaje.
  * **`clasificador_incidencias`**:
      * **Objetivo**: Etiquetar la incidencia en una categor√≠a t√©cnica (Redes, Hardware, etc.).
      * **Modelo recomendado**: **`deepseek-coder`**, por su gran conocimiento de vocabulario t√©cnico.
  * **`buscador_soluciones`**:
      * **Objetivo**: Buscar soluciones y generar un informe detallado.
      * **Modelo recomendado**: **`deepseek-r1`**, por su avanzada capacidad de razonamiento para conectar el problema con las herramientas disponibles.


üß† Documento realizado por un humano y potenciado por IA
